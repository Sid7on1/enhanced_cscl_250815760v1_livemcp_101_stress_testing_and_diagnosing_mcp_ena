{
  "agent_id": "coder2",
  "task_id": "task_6",
  "files": [
    {
      "name": "setup.py",
      "purpose": "Package installation setup",
      "priority": "low"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.CL_2508.15760v1_LiveMCP_101_Stress_Testing_and_Diagnosing_MCP_ena",
    "project_type": "agent",
    "description": "Enhanced AI project based on cs.CL_2508.15760v1_LiveMCP-101-Stress-Testing-and-Diagnosing-MCP-ena with content analysis. Detected project type: agent (confidence score: 9 matches).",
    "key_algorithms": [
      "Instruction",
      "Evaluation",
      "Standardized",
      "Showing",
      "Each",
      "Another",
      "Language",
      "Reinforcement",
      "Plan",
      "Query"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.CL_2508.15760v1_LiveMCP-101-Stress-Testing-and-Diagnosing-MCP-ena.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nLIVEMCP-101: S TRESS TESTING AND DIAGNOSING\nMCP- ENABLED AGENTS ON CHALLENGING QUERIES\nMing Yin1,2\u2217Dinghan Shen2Silei Xu2Jianbing Han2Sixun Dong2Mian Zhang2Yebowen Hu2\nShujian Liu2Simin Ma2Song Wang2Sathish Reddy Indurthi2Xun Wang2\nYiran Chen1\u2020Kaiqiang Song2\u2020\n1Duke University2Zoom Video Communications\nABSTRACT\nTool calling has emerged as a critical capability for AI agents to interact with the\nreal world and solve complex tasks. While the Model Context Protocol (MCP) pro-\nvides a powerful standardized framework for tool integration, there is a significant\ngap in benchmarking how well AI agents can effectively solve multi-step tasks\nusing diverse MCP tools in realistic, dynamic scenarios. In this work, we present\nLiveMCP-101, a benchmark of 101 carefully curated real-world queries, refined\nthrough iterative LLM rewriting and manual review, that require coordinated use\nof multiple MCP tools including web search, file operations, mathematical reason-\ning, and data analysis. Moreover, we introduce a novel evaluation approach that\nleverages ground-truth execution plans rather than raw API outputs, better reflect-\ning the evolving nature of real-world environments. Experiments show that even\nfrontier LLMs achieve a success rate below 60%, highlighting major challenges\nin tool orchestration. Detailed ablations and error analysis further reveal distinct\nfailure modes and inefficiencies in token usage, pointing to concrete directions\nfor advancing current models. LiveMCP-101 sets a rigorous standard for evaluat-\ning real-world agent capabilities, advancing toward autonomous AI systems that\nreliably execute complex tasks through tool use.\n1 I NTRODUCTION\nMCP Pool\nTravel\nLiveMCP -101\n Result\nConstruction\nEvaluation\nSDE\nUser Query\nExecute\nRevise\nEvaluated AgentsReal -Time Ref\nReal -Time Eval\nReference Agent\nFigure 1: Construction and Evaluation framework of LiveMCP-101.\nThe ability to interact with external tools and services is a cornerstone of autonomous AI\nagents (Schick et al., 2023; Qin et al., 2023a), enabling them to extend their capabilities beyond\n\u2217Work done during internship at Zoom\n\u2020Corresponding author.\n1arXiv:2508.15760v1  [cs.CL]  21 Aug 2025\n\n--- Page 2 ---\nstatic knowledge and engage dynamically with the real world. Recent advancements in tool inte-\ngration frameworks\u2014most notably the Model Context Protocol (MCP) (Anthropic, 2024)\u2014have\nstandardized how models discover, invoke, and coordinate tools across diverse domains, from web\nsearch and data analysis to API interactions and file manipulation. These developments promise a\nnew generation of AI agents capable of executing complex, multi-step tasks with minimal human\nintervention (Yao et al., 2022; Shinn et al., 2024). However, reliability remains a key barrier to\nreal-world deployment, as systems that perform well in prototypes often fail on diverse user queries\nand in real production environments (Lu et al., 2024; Yao et al., 2024; Barres et al., 2025).\nUnderstanding why agents fail (Zhang et al., 2025b; Cemri et al., 2025) in realistic, temporally\nevolving production environments can offer valuable insights for improving the corresponding\nmodels and system architectures. However, existing benchmarks (Li et al., 2023a; Tang et al., 2023;\nXu et al., 2023; Patil et al., 2024; Liu et al., 2025) focus only on single-step tool calls, synthetic\nenvironments (normally a mock database), or limited tool sets, failing to capture the complexity and\ndynamism of real-world scenarios. In practice, agents must interact with practical tools that may\nproduce varying responses over time and span entirely different domains. Moreover, user queries can\ncarry nuanced contexts and specific constraints (Zhong et al., 2025), requiring accurate reasoning\nacross dozens of tool calls to complete a task. As a result, current benchmarks cannot fully reveal the\ngaps in current agent systems when deployed in real production environments.\nIn this work, motivated by the goal of stress testing frontier LLMs and agents in realistic, challenging\nscenarios, we introduce LiveMCP-101 \u2014a benchmark of 101 carefully designed tasks requiring\ncoordinated use of diverse MCP-enabled tools (Anthropic, 2024) (e.g., web browsing, file operations,\nmathematical reasoning, data analysis). Notably, user queries are refined through multiple iterations\nof LLM rewriting and manual review (Wang et al., 2022) to enhance complexity while preserving\npracticality, enabling us to reveal where even state-of-the-art agentic models and systems fall short.\nTo ensure robust evaluation\u2014given that MCP-enabled tools may return varying responses to the same\nAPI call over time\u2014we propose a novel setup that runs two agents in parallel, one following the\nground-truth execution plan and the other operating autonomously, and compute a score based on\ntheir real-time outputs.\nOur experiments reveal that even state-of-the-art models struggle with the demands of complex tool\norchestration, achieving less than 60% success rate. It highlights a significant gap between current\nagent capabilities and the robustness required for truly autonomous task execution. More importantly,\nby digging deep into the error cases with various models, we are able to glean useful insights into\ndifferent models\u2019 agentic capabilities. By carefully analyzing the agent trajectories (Chen et al., 2023),\nwe identify seven common failure modes in frontier models, shedding light on how to further improve\nthese systems. Moreover, we observe a striking log-shaped curve in token efficiency: closed-source\nmodels gain rapidly and then plateau, while open-source models fail to turn tokens into reliable\nevidence. We release this benchmark to accelerate development in tool-augmented AI systems and\nfoster innovations in planning, reasoning, and long-horizon task execution (Xi et al., 2023).\nConcurrently, (Liu et al., 2025) also proposes to evaluate AI agents connected to MCP tools and\nservers, but their setup is limited to user queries within a single MCP server and involves low task\ncomplexity. Consequently, different models achieve similar success rates, offering limited insight into\ntheir distinct strengths and weaknesses. Moreover, their evaluation compares the agent\u2019s trajectory\nonly against a static ground-truth execution plan, failing to account for variations in MCP server\nresponses over time, which are critical for assessing robustness in dynamic environments.\nIn summary, our contributions are as follows:\n\u2022We introduce LiveMCP-101 , a benchmark of 101 diverse real-world tasks requiring coordinated\nuse of multiple MCP tools (from diverse domains), with user queries refined through iterative LLM\nrewriting and manual review.\n\u2022We propose a novel evaluation approach that leverages ground-truth execution plans, rather than\nraw API responses or end results, to account for the evolving nature of real-world environments.\n\u2022Experiments show that even frontier LLMs achieve a task success rate below 60% , underscoring\nmajor challenges in real-world tool orchestration.\n\u2022Detailed ablation studies and error analysis draw insights on how to further improve current models,\nfrom the perspectives of different agent failure modes and token efficiency.\n2\n\n--- Page 3 ---\n2 R ELATED WORK\nAgents with Tool Use Chain-of-Thought (CoT) prompting (Wei et al., 2022) first demonstrated that\nlarge language models could remarkably improve performance on complex questions by making their\nintermediate reasoning steps explicit. Its success fueled the rapid rise of reasoning models (OpenAI,\n2024; Guo et al., 2025), which consistently outperform baselines across diverse domains. The\nReAct (Yao et al., 2022) framework extends CoT by decoupling reasoning from tool calls, enabling\nLLM-based agents to ground their reasoning in external information, correct errors mid-process, and\nadapt their plans dynamically. Building on these foundations, recent LLM-agent research explores\ndiverse strategies for autonomous tool use. One line of work focuses on fine-tuning LLMs with\nlarge and diverse tool collections, enabling them to operate effectively over extensive real-world\nAPIs (Schick et al., 2023; Du et al., 2024; Qin et al., 2023b). Another approach adopts modular\nand hierarchical architectures that decompose agents into specialized roles for proposing, planning,\nexecuting, and evaluating, improving robustness and compositional reasoning (Zhuang et al., 2023;\nZhou et al., 2024; Shi et al., 2024). Retrieval-augmented methods further enhance tool access by\nincorporating improved retrieval, documentation compression, and reranking techniques (Yuan et al.,\n2024; Zheng et al., 2024).\nThe emergence of Model Context Protocol (MCP) (Anthropic, 2024) marks a turning point for agentic\ntool use. MCP provides a standardized, JSON-RPC-based API layer for integrating LLMs with\nexternal tools. Since its release, MCP has been quickly adopted across all major AI players and has\nattracted significant attention from the research community (Hou et al., 2025; Ehtesham et al., 2025).\nEvaluation of Agentic Tool Use Evaluating agentic tool use in LLMs presents inherent challenges,\nand substantial research effort has been devoted to developing benchmarks that capture different\ndimensions of this capability. Early work primarily focused on assessing single-turn function-calling\nabilities of LLMs, where the model is required to invoke the correct tool in response to a given\nquery (Yan et al., 2024; Qin et al., 2023b; Guo et al., 2024; Li et al., 2023b; Wu et al., 2024; Patil\net al., 2024). Subsequent benchmarks have extended this scope to multi-turn conversational settings,\nwhere effective tool use requires maintaining context and reasoning across dialogue turns (Wang\net al., 2023; Song et al., 2023; Lu et al., 2024; Yao et al., 2024; Barres et al., 2025).\nFollowing the emergence of MCP, various datasets and benchmarks have been proposed to evaluate\nthe MCP ecosystem. Luo et al. (2025) introduced MCPBench, the first MCP-specific evaluation.\nBuilding on this, Gao et al. (2025) proposed MCP-RADAR, which adopts a multi-dimensional\nevaluation approach; while Liu et al. (2025) presented MCPEval, an automated, fine-grained MCP\nevaluation framework. These early MCP benchmarks have limited scope with only around 10 MCP\nservers. Fei et al. (2025) presented MCP-Tools, a large MCP-tool retrieval dataset (308 servers, 2,797\ntools). However, it does not provide a benchmark for evaluation. A concurrent work by Mo et al.\n(2025) introduced LiveMCPBench, which evaluates agents against dynamic, real-time MCP servers,\nand employs LLM-as-judge evaluator for scoring. However, tasks in LiveMCPBench are relatively\nsimple, averaging only 2.7 tool calls and 2.8 steps per example. In addition, its gold annotations\nspecify only the tool names without detailed parameters, and these annotations are not used as a\nreference during scoring. This leads to results that may not fully capture relative model capabilities,\nfor instance, GPT-4.1-mini outperforming GPT-4.1. In contrast, our LiveMCP-101 benchmark\nintroduces a three-tiered difficulty structure (easy, medium, hard), with tasks requiring an average of\n5.4 tool-calling steps, making it a significantly more challenging benchmark for LLMs. Furthermore,\nwe provide detailed gold-standard tool invocation chains, which serve as explicit references to ensure\nscoring consistency and closer alignment with human judgments.\n3 L IVEMCP-101\n3.1 C ONSTRUCTION\nQuery Generation To generate challenging queries that require agents to leverage tools across\ndifferent domains, we first sample diverse application domains from the overall MCP tool pool,\nwhich spans 41 MCP servers and 260 tools, using GPT-4.1. As the next step, we employ OpenAI o3\nmodel to generate queries of varying complexity, conditioned on domain context and detailed tool\nspecifications (names, descriptions, and parameters). However, even with carefully tuned prompts,\n3\n\n--- Page 4 ---\nsome generated queries are either not solvable with the provided tools or have end states whose\nresults are not easily verifiable. To ensure the dataset is clean and rigorous, we perform multiple\nrounds of LLM rewriting and manual revision (Wang et al., 2022) to guarantee clarity, balanced\ndifficulty, solvability with the given tools, and objectively verifiable outcomes. All queries are divided\ninto three difficulty tiers: Easy (30), Medium (30), and Hard (41). Figure 2 illustrates representative\nexamples from each tier.\nExecution Plan Generation Because tasks interact with live, time-varying MCP services, answers\nmay change over time. Thus, having fixed ground-truth results may not serve as a reliable way to\nevaluate agent results at test time. To resolve this issue, for every curated query, we draft an execution\nplan with o3 given the query and tool specifications. As the next step, we revise it using the reference\nagent\u2019s execution trajectory and outputs, combining LLM-assisted edits with manual adjustments to\ncorrect logical, tool-selection, parameter, and data-processing errors. Approximately 120 PhD-hours\nwere required for this revision. Each task was validated across multiple trials with human verification\nof correctness. The finalized plan deterministically yields the reference output when followed. The\ndistribution of tool-chain lengths in the execution plans is shown in Figure 3.\n3.2 E VALUATION\nEvaluation framework For each task, we launch two parallel executions: (1) a real-time reference\nexecution , where the reference agent strictly follows the validated execution plan using only the MCP\ntools specified therein to produce the reference output; (2) a real-time test execution , where the\nagent being evaluated receives only the natural-language query and a predefined per-task MCP tool\npool, and must independently analyze the query, select tools, schedule calls, and process intermediate\nresults. The test execution proceeds until the agent declares completion or reaches the maximum\niteration rounds. The per-task pool contains all task-essential tools plus a set of additional MCP tools\n(to make the task more challenging for the agents). These additional tools approximate real-world\nchoice breadth, and enable assessment of tool discovery and selection under distractors. This setup\nmitigates temporal drift and enables a fair comparison between the evaluated agent\u2019s output and the\nreference. In addition, the reference trajectory provides a reference for analyzing the evaluated agent\u2019s\ntrajectory, enabling fine-grained diagnosis of planning, tool-selection, parameter, and output-handling\nerrors.\nEvaluation metrics Both the result and trajectory of the evaluated agent are scored by an\nLLM judge (Zheng et al., 2023) using a 1-5 Likert scale (Liu et al., 2023) and then mapped to\n{0.00,0.25,0.50,0.75,1.00}. Prompts are provided in the Appendix A.1 and Appendix A.2.\nResult Metrics : we use Task Success Rate (TSR), the proportion of instances with a score 1.00, and\nAverage Result Score (ARS), the mean score across instances. TSR measures the proportion of tasks\nsuccessfully solved, while ARS reflects the overall quality of the solutions.\nTrajectory Metric : Average Trajectory Score (ATS) of the evaluated agent is reported across all tasks.\nThis metric evaluates trajectories for logical coherence, completeness, and correctness, capturing the\nquality of the solution process complementary to result metrics.\nAverage Token Consumption : in order to measure the token efficiency, for each task, we sum the\nagent\u2019s output tokens across all the iteration rounds; the reported value is the mean of these per-task\ntotals over the evaluation set.\nAverage Tool Calls : for each task, we count the tool invocations across the full trajectory; the reported\nvalue is the mean of these per-task counts over the evaluation set.\n4 E XPERIMENTS\n4.1 E XPERIMENTAL SETUP\nModels We evaluate a diverse set of 18 widely used and popular LLMs on LiveMCP-101: OpenAI\n(GPT-5, GPT-5-mini, GPT-4.1, GPT-4o, GPT-4.1-mini, GPT-4o-mini, o3, o4-mini), Anthropic\n(Claude-4.1-Opus, Claude-4-Sonnet, Claude-3.7-Sonnet), Google (Gemini-2.5-Pro, Gemini-2.5-\nFlash), and open-source (Qwen3-235B-A22B, Qwen3-32B, Qwen3-8B, Llama-3.3-70B-Instruct,\nLlama-3.1-8B-Instruct). For OpenAI reasoning models (OpenAI, 2025b), the reasoning effort is set to\n4\n\n--- Page 5 ---\nEasy\nDuring a recent weekly meeting, my mentor highlighted the need for improved DevOps monitoring. Please prepare a Markdown file named\nk8sissues report.md listing the titles and URLs of the five most recently opened unresolved issues (exclude PRs) from the kubernetes/ku-\nbernetes repository.\nMedium\nAs part of a recent initiative at the fictional consultancy firm BrightPath Analytics, commissioned by the renowned artist Lucia Moretti for an upcoming\nexhibition in Zurich, you are tasked with supporting market research on the digital art landscape. Lucia is specifically interested in public engagement with\nYouTube content for \u2018AI-generated art tools\u2019. Retrieve the first five search results returned for this query. For each video, compute an engagement rate\ndefined as views divided by video duration (in minutes). Compile view counts, video lengths, and engagement rates for the five entries into an Excel file\ntitled youtube aiartvideos.xlsx for forwarding to Lucia\u2019s Zurich studio.\nHard\nMy 9-year-old son is obsessed with his favorite NBA team and keeps giving me cryptic clues. Yesterday at dinner he said, \u201cDad, did you know our team\u2019s\nname owes a huge debt to a Spielberg sci-fi masterpiece?\u201d He\u2019s been begging me to see a home game at their arena. I\u2019d like to surprise him with tickets\nfor a game exactly 60 days from today (local time). We\u2019ll fly in the night before and need accommodation for one night. Since it\u2019s just the two of us\nand we want to be close to the action, please list all available Airbnb properties within a 12-minute brisk walk (assuming 5km/h ) of the team\u2019s home\narena. My budget is strictly $150\u2013 $160 USD per night. Please retrieve official team information and produce a comprehensive Markdown report titled\nnbagame trip.md . This report should present the following information cohesively: first, the exact team name; second, detailed team information\nincluding the team name, conference, division, founded year, home arena, arena location, arena capacity, team colors, and championships; and finally, all\nqualifying accommodation options including the name, listing ID, nightly price, distance to the arena, walking time, and a booking link.\nFigure 2: Example queries by difficulty level. These queries require the multi-step composition of\nheterogeneous MCP tools, with proper parameterization and output handling.\n2 3 4 5 6 7 8 9 11 12 13 15\nT ool Chain Length0510152025Percentage of T asks (%)4.9513.8619.8020.79\n14.85\n13.86\n4.95\n2.97\n0.99 0.99 0.99 0.99\nFigure 3: Distribution of tool-chain lengths in the LiveMCP-101 execution plans.\nmedium. For Anthropic models, we test both standard and extended thinking (ET) models (Anthropic,\n2025). For Qwen3 models, thinking is enabled by default (Qwen Team, 2025).\nSettings Each agent is limited to a maximum of 30 iteration rounds. For reference execution,\nwe employ GPT-4.1 due to its low latency (OpenAI, 2025a) and strong instruction-following capa-\nbilities (OpenAI, 2025a; Zhang et al., 2025a), strictly adhering to the validated execution plan to\nproduce the reference output. For each task, a per-task MCP pool is constructed by combining all\ntask-essential servers with randomly sampled MCP servers, yielding a total of 15 MCP servers and\n76\u2013125 tools available per task. We adopt the widely used ReAct prompting (Yao et al., 2023) and\nuse GPT-4.1 as the LLM judge (Zheng et al., 2023) to score both the final output and the execution\ntrajectory.\nMetrics As described in our evaluation framework, we report the following metrics for each model:\ntask success rate (TSR), average result score (ARS), average trajectory score (ATS), average tool\ncalls and average tokens used.\n5\n\n--- Page 6 ---\n4.2 M AINRESULTS\nAs shown in Table 1, GPT-5 achieves the best overall performance on LiveMCP-101, leading across\nall difficulty tiers. Next are o3, GPT-5-mini, Claude-4.1-Opus (ET), and Claude-4-Sonnet (ET),\nindicating that stronger reasoning effort can yield meaningful improvements for dynamic, multi-step\nproblem-solving and MCP tool calling. Among mid-tier proprietary models, GPT-4.1, Gemini-2.5-\nPro, and Claude-3.7-Sonnet perform reasonably well but remain behind the top performers. Open-\nsource models lag behind proprietary models. Among them, Qwen3-235B-A22B (22.77%/42.57%)\nis the strongest among them yet remains far from the frontier. Llama models underperform notably\non LiveMCP-101, and a more detailed analysis is provided in Section 5.2. Performance degrades\nsubstantially with task difficulty for all models. Notably, even the strongest model attains only 39.02%\nTSR on Hard. Rankings by TSR and ARS are broadly consistent.\nFigure 4a visualizes the relationship among TSR, ARS, and ATS. The color-encoded ATS increases\nwith both ARS and TSR, with higher-ATS models clustering toward the upper-right region. This\nsuggests that better trajectories usually yield better outputs. Higher ATS corresponds to more reliable\ntool selection, parameterization, and post-processing, which helps satisfy task success criteria. Figure\n4b shows the relationship between TSR, the average number of tokens, and the average number\nof tool calls. Closed-source models exhibit a mild upward trend with tokens, yet planning quality\nremains the primary driver.Open-source models exhibit two characteristic inefficiencies. Llama\nvariants cluster in the low-token, low-tool region, under-exploring tool affordances and often stopping\nearly, which yields low ARS and TSR. Qwen variants trend toward the opposite extreme, producing\nlonger outputs and invoking more tools without commensurate gains compared to the closed-source\nmodels. Extended-thinking variants consistently shift the efficiency frontier upward at comparable\ntoken budgets, suggesting gains from improved planning and error recovery rather than verbosity.\nModelOverall Easy Medium Hard\nTSR ARS TSR ARS TSR ARS TSR ARS\nGPT-5 58.42 73.02 86.67 89.17 56.67 72.50 39.02 61.59\no3 46.53 64.60 66.67 80.00 46.67 65.83 31.71 52.44\nGPT-5-mini 43.56 63.12 63.33 82.50 43.33 64.17 29.27 48.17\nClaude-4.1-Opus (ET) 41.58 61.88 56.67 79.17 43.33 61.67 29.27 49.39\no4-mini 40.59 61.63 53.33 77.50 46.67 62.50 26.83 49.39\nClaude-4-Sonnet (ET) 43.56 60.40 63.33 79.17 46.67 62.50 26.83 45.12\nClaude-4.1-Opus 39.60 59.41 60.00 83.33 33.33 49.17 29.27 49.39\nClaude-4-Sonnet 37.62 55.69 63.33 78.33 46.67 65.00 12.20 32.32\nGPT-4.1 35.64 55.94 60.00 76.67 36.67 55.83 17.07 40.85\nClaude-3.7-Sonnet (ET) 29.70 47.77 43.33 66.67 26.67 46.67 21.95 34.76\nGemini-2.5-Pro 27.72 46.78 36.67 61.67 30.00 46.67 19.51 35.98\nClaude-3.7-Sonnet 26.73 42.57 46.67 61.67 20.00 40.83 17.07 29.88\nQwen3-235B-A22B 22.77 42.57 43.33 63.33 26.67 45.00 4.88 25.61\nGPT-4o 21.78 41.09 40.00 62.50 20.00 37.50 9.76 28.05\nGPT-4.1-mini 17.82 35.15 36.67 56.67 13.33 31.67 7.32 21.95\nQwen3-32B 18.81 34.41 36.67 59.17 16.67 32.50 7.32 17.68\nGPT-4o-mini 8.91 27.48 16.67 40.83 6.67 31.67 4.88 14.63\nGemini-2.5-Flash 10.89 22.48 26.67 44.17 10.00 22.33 0.00 6.71\nQwen3-8B 3.96 11.63 10.00 26.67 3.33 8.33 0.00 3.05\nLlama-3.3-70B-Instruct 1.98 6.93 3.33 15.83 3.33 5.83 0.00 1.22\nLlama-3.1-8B-Instruct 0.99 2.72 3.33 9.17 0.00 0.00 0.00 0.00\nTable 1: Task success rate (TSR, %) and average result score (ARS, %) overall and by difficulty\n(Easy/Medium/Hard). Shaded rows mark the top-3 models by overall TSR. Bold indicates column-\nbest. ET denotes extended thinking enabled for Anthropic models.\n6\n\n--- Page 7 ---\n0 10 20 30 40 50 60 70 80\nAverage Result Score (ARS, %)0102030405060T ask Success Rate (TSR, %)\n2030405060\nAverage Trajectory Score (ATS, %)(a)\n0 3000 6000 9000 12000 15000 18000\nAverage T okens0102030405060T ask Success Rate (TSR, %)\n46810121416\nAverage T ools (b)\nFigure 4: Results on LiveMCP-101, showing model performance in terms of task success rate (TSR),\naverage result score (ARS), average trajectory score (ATS), average token consumption, and average\ntool calls. (a)TSR (%) vs. ARS (%), with color encoding ATS (%). (b)TSR (%) vs. average tokens\nper task, with color encoding average tool calls.\n4.3 A BLATION STUDY\nWe conduct ablations on GPT-5, Claude-4.1-Opus (ET), GPT-4.1, Gemini-2.5-Pro, Qwen3-235B-\nA22B, and Qwen3-8B, covering frontier and mid-tier closed-source models and open-source models.\nImpact of maximum iteration rounds In LiveMCP-101, the longest validated execution plan\nrequires 15 tool calls. By default, each agent is limited to 30 iteration rounds, where each round may\ninvolve one or more tool invocations. To study sensitivity to the iteration budget, we vary this limit to\n15, 20, 30, and 50 rounds. The results in Figure 5 (a) and (b) highlight two key phenomena. First,\nincreasing the max iteration limit from 15 to about 25 rounds consistently improves task success, as\nthe added budget enables more thorough tool exploration and error recovery (Yuan et al., 2025; Zhang\net al., 2025c). Notably, although the longest validated execution plan comprises 15 tool calls (with\nan average of 5.4), the continued gains when raising the round limit from 15 to around 25 indicate\nthat agents often expend extra rounds on error recovery or redundant deliberation even on correctly\nsolved instances, revealing substantial headroom for execution efficiency. Second, beyond 25 rounds,\nthe benefits saturate: performance becomes constrained by model capability\u2014particularly planning\nquality and tool-use competence\u2014rather than iteration capacity. Additional rounds yield diminishing\nreturns and can even introduce noise or compound errors, leaving performance essentially flat.\n15 20 25 30 50\nRounds2030405060TSR (%)\n(a)\n15 20 25 30 50\nRounds05101520Relative Change (%)(b)\n6 8 10 12 15\nMCP servers2030405060TSR (%)\n(c)\n6 8 10 12 15\nMCP servers20\n15\n10\n5\n0Relative Change(%)(d)GPT-5 Claude-4.1-Opus (ET) GPT-4.1 Gemini-2.5-Pro Qwen3-235B-A22B Qwen3-32B\nFigure 5: Ablation study results. (a)TSR (%) vs. max iteration rounds: all models improve from 15\nto around 25 rounds, then plateau. (b)Relative TSR change w.r.t. 15-round setting shows diminishing\nreturns beyond about 25. (c)TSR (%) vs. number of MCP servers: top-tier models remain largely\nstable, while weaker or mid-tier models degrade as distractors grow. (d)Relative change w.r.t.\n6-server setting shows that larger pools affect weaker models more, consistent with long-context\nsensitivity and tool-selection noise.\n7\n\n--- Page 8 ---\nImpact of the number of MCP servers In the default setting, the most demanding task requires up\nto 6 MCP servers, and we expose a pool of 15 servers to the evaluated agent. To study sensitivity to\nMCP server breadth, we vary the pool size to 6, 10, 12, and 15. We set 15 as the upper limit because\nlarger pools could hit API limits (e.g., 128 tools per request) (OpenAI, 2025) or exceed context\nlength. This choice keeps the setup realistic and comparable to real-world deployments. As the pool\ngrows, the expanded tool search and tool call space increases selection overhead and the likelihood of\nspurious tool usage. We find weaker and mid-tier models more sensitive to this effect, often showing\ndeclines as noise accumulates and planning bandwidth is diluted. In contrast, top-tier systems (e.g.,\nGPT-5, Claude-4.1-Opus (ET)) remain largely stable: stronger planning and tool-screening mitigate\ndistractors, so performance changes are negligible.\n4.4 A NALYSIS OF LLM- AS-A-JUDGE\nGPT-5\nClaude-4.1-Opus (ET)GPT-4.1\nGemini-2.5-Pro\nQwen3-235B-A22BQwen3-32B050100Cohen's  (%)\n87.5 88.2 86.9 86.4 85.1 87.380.8 79.2 81.5 78.4 81.1 79.9Result Trajectory\nFigure 6: Human\u2013LLM agreement (Cohen\u2019s \u03ba, %)\non result and trajectory evaluation for six models.\nBlue bars denote scores for the result evaluation, and\npink bars denote scores for the trajectory evaluation.We apply an LLM-as-a-Judge to score both\nfinal outputs and execution trajectories. To as-\nsess reliability, we conduct a blinded human-\nexpert study on a stratified subset of tasks for\nsix representative models: GPT-5, Claude-4.1-\nOpus (ET), GPT-4.1, Gemini-2.5-Pro, Qwen3-\n235B-A22B, and Qwen3-32B. Experts follow\nthe same rubric and judge prompts. We com-\npare human and LLM-judge decisions at the\ninstance level and report inter-rater agreement\nusing Cohen\u2019s \u03ba(Cohen, 1960). We evaluate\na sampled set of 30 tasks in total with 10 per\ndifficulty tier (Easy, Medium, Hard). Across\nall six models, the human vs. LLM-judge agreement (quadratic-weighted Cohen\u2019s \u03ba) exceeds 85%\nfor the result evaluations and 78% for the trajectory evaluations respectively, indicating that the LLM\njudge yields consistent, human-aligned ratings (Landis & Koch, 1977; Fleiss, 1981).\n5 D ISCUSSION\n5.1 T OKEN EFFICIENCY\nWe observe that closed-source models exhibit a striking log-shaped pattern : task success rate (TSR)\nrises rapidly with small token budgets, then plateaus (Figure 4b). Intuitively, early tokens drive\nhigh-value actions\u2014planning, probing tools, checking constraints\u2014yielding large gains. But as\nbudgets grow, extra tokens mostly add redundancy (longer explanations, repeated self-checks) rather\nthan new evidence, and returns diminish. This curve reflects models\u2019 token efficiency: even the\nstrongest struggle to make effective use of additional tokens beyond a certain point. How to maximize\nintelligence per token, or per dollar, remains an intriguing open challenge for MCP-based agents.\nIn contrast, open-source models break this trend: despite equal or greater token use, TSR barely\nimproves, revealing a failure to turn tokens into reliable evidence and thus lower token efficiency.\n5.2 F AILURE ANALYSIS\nTo diagnose failure modes in MCP-based tool use, we carefully analyze execution logs across\ndifferent models and identify three error categories including seven subtypes: tool planning and\norchestration errors (1\u20134), parameter errors (5\u20136), and output handling errors (7).\n(1)Ignoring requirement : the agent misses an explicitly stated requirement and does not select\nany relevant tool. Typical signs include no corresponding thinking process and tool call, early\ntermination, or a generic final answer that does not address the requirement. This often occurs\nwhen the agent fails to extract key requirements from the prompt or loses track of them during\nexecution. (2) Overconfident self-solving : the agent recognizes the requirement but attempts to\nanswer from its own knowledge or using its own reasoning and capabilities without calling the\nneeded tool. Symptoms include: no corresponding tool call, generic or hallucinated answers, and\npremature termination. (3) Unproductive thinking : the agent acknowledges that a tool is needed\nand may discuss plans or parameters, but never initiates the call and does not propose any solution\nthat addresses the requirement. It loops in unproductive or verbose thinking and eventually times\nout or gives up. Symptoms include repeated plan rewrites without execution, token-consuming\n8\n\n--- Page 9 ---\nthinking, and reaching the round limit with zero calls for the requirement. (4) Wrong tool selection :\nthe agent calls a tool but chooses an inappropriate one, leading to erroneous intermediate states or\nfinal outputs. This can happen as a single misselection or repeated wrong calls until the budget is\nexhausted. Symptoms include irrelevant responses, repeated mistakes, or missing required fields in\noutputs. (5) Syntactic errors : parameters provided to a tool are malformed, such as having incorrect\ntypes, missing or wrong field names, or invalid schema. These errors prevent the MCP server from\ncorrectly parsing the request, leading to failure. (6) Semantic errors : parameters are well-formed\nbut do not match the task intent. Common cases include mis-scoped query strings, wrong identifiers\nor entity references, and incorrect contextual constraints. These errors often arise from mistakes in\nintermediate reasoning used to generate parameters. (7) Output parsing errors : the tool returns a\ncorrect result, but the agent mishandles it during parsing, causing incorrect intermediate states or final\nanswers. We further evaluate several popular models spanning a range of capabilities, and their core\nerror-type distributions are shown in Figure 7. Several interesting observations can be inferred from\nthe results:\n\u2022Semantic errors dominate: even strong models show rates of 16\u201325%, while smaller ones exceed\n40% (e.g., GPT-4.1-mini), pinpointing content grounding and constraint enforcement as the primary\nbottleneck in live tool use.\n\u2022Syntactic errors are negligible for frontier models but catastrophic for Llama-3.3-70B-Instruct\n\u224848%. A likely cause is limited MCP-specific training\u2014MCP adoption surged (Ehtesham et al.,\n2025) after the Llama-3 release (Meta Llama Team, 2024)\u2014suggesting that targeted fine-tuning on\nMCP function-call schemas could substantially cut such errors and boost overall performance.\n\u2022Overconfident self-solving is common in mid-tier models: they often skip tool calls because\nplanning and screening remain brittle under large tool pools and long contexts, making reliance\non internal knowledge (Chhikara, 2025) seem safer than attempting uncertain tool selection and\nparameterization.\nCorrect\nIgnoring requirement\nOverconfident self-solvingUnproductive thinkingWrong tool selectionSyntactic errors Semantic errors\nOutput parsing errorsgpt-5\no3\nclaude-4.1-opus (ET)\nclaude-4.1-opus\nclaude-4-sonnet\ngpt-4.1\ngemini-2.5-pro\nqwen3-235b\ngpt-4.1-mini\nllama3-70b58.42 4.95 3.96 2.97 6.93 0.00 16.83 5.94\n46.53 4.95 4.95 5.94 8.91 0.00 17.82 11.88\n41.58 5.94 8.91 4.95 4.95 0.00 22.77 10.89\n39.60 5.94 9.90 5.94 5.94 0.00 23.76 8.91\n37.62 7.92 10.89 4.95 6.93 0.00 22.77 8.91\n35.64 9.90 6.93 0.99 7.92 0.00 24.75 13.86\n27.72 19.80 5.94 8.91 8.91 1.98 20.79 7.92\n22.77 13.86 7.92 6.93 9.90 0.99 27.72 9.90\n17.82 8.91 5.94 4.95 10.89 0.99 41.58 8.91\n1.98 8.91 10.89 6.93 6.93 48.51 12.87 2.97\n0.0010.0020.0030.0040.0050.0060.00\nPercentage of all tasks (%)\nFigure 7: Error classification heatmap across models. The leftmost column (Correct) aligns with\nTSR, while the remaining columns decompose failures into 7 fine-grained subtypes.\n6 C ONCLUSION\nIn this work, we introduce LiveMCP-101 , a benchmark of challenging user queries for evaluating\nAI agents\u2019 ability to plan and execute multi-step tool use in realistic, dynamic environments via the\nModel Context Protocol (MCP). The benchmark comprises 101 diverse tasks, refined through iterative\nLLM-based rewriting and manual review, spanning domains including web search, file operations,\nmathematical reasoning, and data analysis. We further propose an evaluation methodology based on\nground-truth execution plans, providing a more reliable measure of agent performance in evolving\nenvironments. Experiments show that even frontier LLMs achieve a success rate below 60%,\nunderscoring key challenges in tool orchestration, adaptive reasoning, and token efficiency. Detailed\nablations and error analysis reveal distinct failure modes and highlight opportunities for improvement.\nBy releasing LiveMCP-101, we establish a rigorous and scalable framework to benchmark and\nadvance the development of more capable autonomous AI agents.\n9\n\n--- Page 10 ---\nREFERENCES\nAnthropic. Model context protocol, 2024. URL https://modelcontextprotocol.io/ .\nAnthropic. Extended thinking in claude. https://docs.anthropic.com/en/docs/\nbuild-with-claude/extended-thinking , 2025. ET (extended thinking) models and\nusage; Accessed 2025-08-21.\nVictor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. \u03c42-bench: Evaluating\nconversational agents in a dual-control environment. arXiv preprint arXiv:2506.07982 , 2025.\nMert Cemri, Melissa Z Pan, Shuyi Yang, Lakshya A Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt\nKeutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, et al. Why do multi-agent llm\nsystems fail? arXiv preprint arXiv:2503.13657 , 2025.\nQiantong Chen, Hongyu Zhang, Xueliang Liu, Jian Feng, Longtao Wang, et al. Tooleval:\nAn automatic evaluation framework for tool-augmented language models. arXiv preprint\narXiv:2307.10813 , 2023.\nPrateek Chhikara. Mind the confidence gap: Overconfidence, calibration, and distractor effects in\nlarge language models. arXiv preprint arXiv:2502.11028 , 2025.\nJ. Cohen. A coefficient of agreement for nominal scales. Educational and Psychological Measurement ,\n20(1):37\u201346, 1960.\nYu Du, Fangyun Wei, and Hongyang Zhang. Anytool: Self-reflective, hierarchical agents for\nlarge-scale api calls. arXiv preprint arXiv:2402.04253 , 2024.\nAbul Ehtesham, Aditi Singh, Gaurav Kumar Gupta, and Saket Kumar. A survey of agent interoperabil-\nity protocols: Model context protocol (mcp), agent communication protocol (acp), agent-to-agent\nprotocol (a2a), and agent network protocol (anp). arXiv preprint arXiv:2505.02279 , 2025.\nXiang Fei, Xiawu Zheng, and Hao Feng. Mcp-zero: Proactive toolchain construction for llm agents\nfrom scratch. arXiv preprint arXiv:2506.01056 , 2025.\nJoseph L. Fleiss. Statistical methods for rates and proportions . John Wiley & Sons, New York, 2nd\nedition, 1981.\nXuanqi Gao, Siyi Xie, Juan Zhai, Shqing Ma, and Chao Shen. Mcp-radar: A multi-dimensional\nbenchmark for evaluating tool use capabilities in large language models. arXiv preprint\narXiv:2505.16700 , 2025.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms\nvia reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\nZhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong\nSun, and Yang Liu. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of\nlarge language models. arXiv preprint arXiv:2403.07714 , 2024.\nXinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. Model context protocol (mcp): Landscape,\nsecurity threats, and future research directions. arXiv preprint arXiv:2503.23278 , 2025.\nJ Richard Landis and Gary G Koch. The measurement of observer agreement for categorical data.\nBiometrics , 33(1):159\u2013174, 1977.\nMinghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank:\nA comprehensive benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244 , 2023a.\nMinghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang,\nand Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms. arXiv preprint\narXiv:2304.08244 , 2023b.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg\nevaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634 , 2023.\n10\n\n--- Page 11 ---\nZhiwei Liu, Jielin Qiu, Shiyu Wang, Jianguo Zhang, Zuxin Liu, Roshan Ram, Haolin Chen, Weiran\nYao, Huan Wang, Shelby Heinecke, et al. Mcpeval: Automatic mcp-based deep evaluation for ai\nagent models. arXiv preprint arXiv:2507.12806 , 2025.\nJiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix Bai, Shuang Ma, Shen\nMa, Mengyu Li, Guoli Yin, et al. Toolsandbox: A stateful, conversational, interactive evaluation\nbenchmark for llm tool use capabilities. arXiv preprint arXiv:2408.04682 , 2024.\nZhiling Luo, Xiaorong Shi, Xuanrui Lin, and Jinyang Gao. Evaluation report on mcp servers. arXiv\npreprint arXiv:2504.11094 , 2025.\nMeta Llama Team. Llama-3.3-70b-instruct: Model card. https://huggingface.co/\nmeta-llama/Llama-3.3-70B-Instruct , 2024. Accessed 2025-08-21.\nGuozhao Mo, Wenliang Zhong, Jiawei Chen, Xuanang Chen, Yaojie Lu, Hongyu Lin, Ben He,\nXianpei Han, and Le Sun. Livemcpbench: Can agents navigate an ocean of mcp tools? arXiv\npreprint arXiv:2508.01780 , 2025.\nOpenAI. Openai o1 system card, 2024. URL https://openai.com/index/\nopenai-o1-system-card/ . Accessed: 2024-12-01.\nOpenAI. Assistants api deep dive. https://platform.openai.com/docs/assistants/\ndeep-dive , 2025. \u201cUse the tools parameter to give the Assistant access to up to 128 tools.\u201d.\nOpenAI. Gpt-4.1. https://openai.com/index/gpt-4-1/ , 2025a. Accessed 2025-08-21.\nOpenAI. Openai platform: Reasoning models. https://platform.openai.com/docs/\nguides/reasoning , 2025b. Describes reasoning effort levels (low/medium/high); Accessed\n2025-08-21.\nShishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language\nmodel connected with massive apis. Advances in Neural Information Processing Systems , 37:\n126544\u2013126565, 2024.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru\nTang, Bill Qian, et al. Tool learning with foundation models. arXiv preprint arXiv:2304.08354 ,\n2023a.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru\nTang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world\napis. arXiv preprint arXiv:2307.16789 , 2023b.\nQwen Team. Qwen3: Think deeper, act faster. https://qwenlm.github.io/blog/qwen3/ ,\n2025. Accessed 2025-08-21.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess `\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761 , 2023.\nZhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Pengjie\nRen, Suzan Verberne, and Zhaochun Ren. Learning to use tools via cooperative and interactive\nagents. arXiv preprint arXiv:2403.03031 , 2024.\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion:\nLanguage agents with verbal reinforcement learning. Advances in Neural Information Processing\nSystems , 36, 2024.\nYifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang,\nCheng Li, Ke Wang, Rong Yao, et al. Restgpt: Connecting large language models with real-world\nrestful apis. arXiv preprint arXiv:2306.06624 , 2023.\nQiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. Toolal-\npaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint\narXiv:2306.05301 , 2023.\n11\n\n--- Page 12 ---\nXingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji.\nMint: Evaluating llms in multi-turn interaction with tools and language feedback. arXiv preprint\narXiv:2309.10691 , 2023.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Sharan, Aakanksha Goodman,\nand Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv\npreprint arXiv:2203.11171 , 2022.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems , 35:24824\u201324837, 2022.\nMengsong Wu, Tong Zhu, Han Han, Chuanyuan Tan, Xiang Zhang, and Wenliang Chen. Seal-tools:\nSelf-instruct tool learning dataset for agent tuning and detailed benchmark. In CCF International\nConference on Natural Language Processing and Chinese Computing , pp. 372\u2013384. Springer,\n2024.\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe\nWang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents:\nA survey. arXiv preprint arXiv:2309.07864 , 2023.\nCheng Xu, Dazhen Guo, Nan Duan, and Julian McAuley. Tool learning with large language models:\nA survey. arXiv preprint arXiv:2405.17935 , 2023.\nFanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and\nJoseph E. Gonzalez. Berkeley function calling leaderboard. 2024.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 ,\n2022.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. In International Conference on\nLearning Representations (ICLR) , 2023.\nShunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. \u03c4-bench: A benchmark for\ntool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045 , 2024.\nSiyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, and\nDeqing Yang. Easytool: Enhancing llm-based agents with concise tool instruction. arXiv preprint\narXiv:2401.06201 , 2024.\nSiyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, and Jiecao Chen. Agent-r: Training\nlanguage model agents to reflect via iterative self-training. arXiv preprint arXiv:2501.11425 , 2025.\nMian Zhang, Shujian Liu, Sixun Dong, Ming Yin, Yebowen Hu, Xun Wang, Steven Ma, Song Wang,\nSathish Reddy Indurthi, Haoyun Deng, et al. Complex logical instruction generation. arXiv\npreprint arXiv:2508.09125 , 2025a.\nShaokun Zhang, Ming Yin, Jieyu Zhang, Jiale Liu, Zhiguang Han, Jingyang Zhang, Beibin Li,\nChi Wang, Huazheng Wang, Yiran Chen, et al. Which agent causes task failures and when? on\nautomated failure attribution of llm multi-agent systems. arXiv preprint arXiv:2505.00212 , 2025b.\nZhisong Zhang, Tianqing Fang, Kaixin Ma, Wenhao Yu, Hongming Zhang, Haitao Mi, and Dong\nYu. Enhancing web agents with explicit rollback mechanisms. arXiv preprint arXiv:2504.11788 ,\n2025c.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. Advances in neural information processing systems , 36:46595\u201346623, 2023.\nYuanhang Zheng, Peng Li, Wei Liu, Yang Liu, Jian Luan, and Bin Wang. Toolrerank: Adaptive and\nhierarchy-aware reranking for tool retrieval. arXiv preprint arXiv:2403.06551 , 2024.\n12\n\n--- Page 13 ---\nLucen Zhong, Zhengxiao Du, Xiaohan Zhang, Haiyi Hu, and Jie Tang. Complexfuncbench: ex-\nploring multi-step and constrained function calling under long-context scenario. arXiv preprint\narXiv:2501.10132 , 2025.\nYifei Zhou, Qianlan Yang, Kaixiang Lin, Min Bai, Xiong Zhou, Yu-Xiong Wang, Sergey Levine,\nand Erran Li. Proposer-agent-evaluator(pae): Autonomous skill discovery for foundation model\ninternet agents. 2024. URL https://arxiv.org/abs/2412.13194 .\nYuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A Rossi, Somdeb\nSarkhel, and Chao Zhang. Toolchain*: Efficient action space navigation in large language models\nwith a* search. arXiv preprint arXiv:2310.13227 , 2023.\n13\n\n--- Page 14 ---\nA P ROMPTS\nA.1 R ESULT EVALUATION PROMPT\nYou are a senior evaluator judging how well an AI agent solves a task.\nTask Query: {query}.\nEV ALUATION INSTRUCTION: Given the query above and the reference answer, evaluate how well the agent solves the task.\nLIKERT-STYLE DISCRETE SCORING (1\u20145)\n- 5 (Excellent): Agent output conveys the same results and information as reference; task fully satisfied; differences in formatting or\nwording are fine\n- 4 (Good): Mostly correct with minor omissions or small inaccuracies\n- 3 (Fair): About half of the results is correct but some requirement not met or with noticeable inaccuracies\n- 2 (Poor): Only a small portion is correct, substantially incomplete or with significant inaccuracies\n- 1 (Fail): No correct or relevant results (off-topic, fabricated, or entirely incorrect)\nYou MUST snap to one of these exact Likert values: 1, 2, 3, 4, or 5.\nCRITICAL RULES:\n1. DO NOT excuse material differences due to \u201cdynamic data\u201d or \u201ctiming\u201d\n2. Focus on the content in both the reference and agent output that fulfills the Query\u2019s requirements and intent.\n3. Structure and wording variations are acceptable\n4. NUMERICAL TOLERANCE: For values that may easily fluctuate briefly (e.g., driving times, prices, view counts):\n- Minor variations plausibly due to short-term fluctuation should be considered correct\n- Example: \u201c$120\u201d vs \u201c$118\u201d for a specific room price is acceptable\n- Example: \u201c25 minutes drive\u201d vs \u201c23\u201427 minutes\u201d is acceptable\n- Example: \u201c6900 view counts\u201d vs \u201c6908 view counts\u201d is acceptable\nProvide your evaluation in the following JSON format:\n{\n\"likert\": <integer 1-5>,\n\"feedback\": \"Detailed explanation for the chosen rating\"\n}\nA.2 T RAJECTORY EVALUATION PROMPT\nYou are a senior evaluator judging the overall quality of the agent\u2019s tool chain (trajectory) for solving the task.\nReference Tool Chain (for context):\n{reference tool chain }\nAgent\u2019s Actual Tool Chain:\n{agent\u2019s actual tool chain }\nLIKERT-STYLE DISCRETE SCORING (1\u20145):\n- 5 (Excellent): The trajectory is logically sound, efficient, complete, and demonstrates strong reasoning. All necessary steps are present, no\nmajor mistakes, and the approach is either optimal or a clearly valid alternative\n- 4 (Good): The trajectory is mostly correct, reasonable, and relevant; steps are generally appropriate and accurate, with noticeable but\nnon-critical omissions or inefficiencies; no critical errors\n- 3 (Fair): Some correct, relevant steps, but with gaps in logic/completeness or several questionable/inefficient choices\n- 2 (Poor): Few correct steps; substantially incomplete or contains clearly wrong tool usage that undermines progress\n- 1 (Failed): The trajectory does not include any correct or relevant steps toward solving the task, is illogical or largely incorrect, and does\nnot meaningfully advance the task; not directly usable\nYou MUST snap to one of these exact Likert values: 1, 2, 3, 4, or 5.\nNote: The agent\u2019s approach does not need to match the reference exactly. Please focus on the overall quality, efficiency, and logic of the\nagent\u2019s tool chain.\nProvide your evaluation in the following JSON format:\n{\n\"likert\": <integer 1-5>,\n\"feedback\": \"Detailed explanation for the chosen rating\"\n}\n14",
  "project_dir": "artifacts/projects/enhanced_cs.CL_2508.15760v1_LiveMCP_101_Stress_Testing_and_Diagnosing_MCP_ena",
  "communication_dir": "artifacts/projects/enhanced_cs.CL_2508.15760v1_LiveMCP_101_Stress_Testing_and_Diagnosing_MCP_ena/.agent_comm",
  "assigned_at": "2025-08-22T21:08:55.990555",
  "status": "assigned"
}